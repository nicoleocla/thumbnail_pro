{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install matplotlib -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install q- datasets\n",
    "!pip install transformers -q -U\n",
    "!pip install -q bitsandbytes sentencepiece accelerate loralib\n",
    "!pip install -q -U git+https://github.com/huggingface/perft.git\n",
    "!pip install hf_transfer -q -U\n",
    "!pip install pickleshare -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"LLaVA\"):\n",
    "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
    "else:\n",
    "    print(\"LLaVA directory already exists. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Define the path to the builder.py file\n",
    "file_path = 'LLaVA/llava/model/builder.py'\n",
    "\n",
    "#Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "#Regular expression to find the block between 'vision_tower = model.get....\n",
    "pattern_block = (\n",
    "    r'(vision_tower = model.get_vision_tower\\(\\)\\n)'\n",
    "    r'.*?' #non-greedy match for any characters\n",
    "    r'(image_processor = vision_tower.image_processor)'\n",
    ")\n",
    "\n",
    "replacement_block = (\n",
    "    r'\\1' # keep starting line unchaged\n",
    "    '     if not vision_tower.is_loader:\\n'\n",
    "    '       print(\\'vision_tower is not loaded so loading it now\\')\\n'\n",
    "    '       vision_tower.load_model(device_map=device_map)\\n'\n",
    "    '       vision_tower.to(deice=device, dtype=torch.bfloat16)\\n'\n",
    "    '     else:\\n'\n",
    "    '       pint(\\'vision_tower is loaded\\')\\n'\n",
    "    r'    \\2' #keep the ending line unchanged\n",
    ")\n",
    "\n",
    "#replace the specific block\n",
    "content = re.sub(pattern_block, replacement_block, content, flegs=re.DOTALL)\n",
    "\n",
    "#Write the modified content back to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(content)\n",
    "print('The script has been updated successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_path = 'LLaVA/llava/model/builder.py'\n",
    "\n",
    "#read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    concept = file.read()\n",
    "    \n",
    "#regular expression to find 'float16' not preceded by 'b'\n",
    "pattern = r'(?<!b)float16'\n",
    "\n",
    "#check if there are any matches\n",
    "if re.search(pattern, content):\n",
    "    #Replace 'float16' with 'bfloat16'\n",
    "    modified_content = re.sub(pattern, 'bfloat16', content)\n",
    "    \n",
    "    #Write the modified contnet back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_content)\n",
    "    \n",
    "    print(\"All necessary instances of floats have been replaced with..\")\n",
    "else:\n",
    "    print('No replacement needed. All instances of float16 already have.. ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can take up to 5 mins\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git pull\n",
    "# !pip install -e . -q\n",
    "\n",
    "!pip install protobuf -q -U\n",
    "!pip install --upgrade Pillow -q\n",
    "!pip install -e \".[train]\" -q\n",
    "!pip install flash-attn --no-build-isolation -q \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "import transformers\n",
    "from transformers import AutoProcessor, Trainer, TrainingArgument, BitsA\n",
    "import torchvision.transforms as transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = 'liuhaotian/llava-v1.6-mistral-7b'\n",
    "#model_path = \"Trelis/llava-v1.6-mistral-7b-PATCHED\"\n",
    "\n",
    "model_name=get_model_name_from_path(model_path)\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,  \n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.config)\n",
    "#print(tokenizer.pad_token_id)\n",
    "#print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#check dtype of all modules, focusing on those not torch.bfloat16\n",
    "print(\"Modules not torch.bfloat16:\")\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'parameters') and list(module.parameters()):\n",
    "        #check if any parameter of the module is not bfloat16\n",
    "        if any(param.dtype != torch.bfloat16 for para in module.parameters()):\n",
    "            print(f\"{name}: {next(module.parameters()).dtype}\")\n",
    "    else:\n",
    "        #Optionally, acknowledge module without parameters if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from llava.constants import (\n",
    "  IMAGE_TOKEN_INDEX,\n",
    "  DEFAULT_IMAGE_TOKEN,\n",
    "  DEFAULT_IM_START_TOKEN,\n",
    "  DEFAULT_IM_END_TOKEN,\n",
    "  IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.converters import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "  process_images,\n",
    "  tokenizer_image_token,\n",
    "  get_model_name_from_path,\n",
    ")\n",
    "\n",
    "# MODIFY THIS TO YOUR OWN MODEL\n",
    "# CAPTION SHOULD BE CHANGED TO THE CORRESPONDING MODEL\n",
    "def create_prompt(query, model, model_name=model_name, caption=None):\n",
    "  image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "  if IMAGE_PLACEHOLDER in query:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "      query = re.sub(IMAGE_PLACEHOLDER, image_token_se, query)\n",
    "    else:\n",
    "      query = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, query)\n",
    "  else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "      query = image_token_se + \"\\n\" + query\n",
    "    else:\n",
    "      query = DEFAULT_IMAGE_TOKEN + \"\\n\" + query\n",
    "  \n",
    "  conv_mode = infer_conv_mode(model_name)\n",
    "  conv = conv_templates[conv_mode].copy()\n",
    "  conv.append_message(conv.roles[0], query)\n",
    "  if caption is not None:\n",
    "    conv.append_message(conv.roles[1], caption)\n",
    "  else:\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "  return conv.get_prompt()\n",
    "\n",
    "def infer_conv_mode(model_name):\n",
    "  if \"llama-2\" in model_name.lower():\n",
    "    return \"llava_llama_2\"\n",
    "  elif \"mistral\" in model_name.lower():\n",
    "    return \"mistral_instruct\"\n",
    "  elif \"v1.6-34b\" in model_name.lower():\n",
    "    return \"chatml_direct\"\n",
    "  elif \"v1\" in model_name.lower():\n",
    "    return \"llava_v1\"\n",
    "  elif \"mpt\" in model_name.lower():\n",
    "    return \"mpt\"\n",
    "  else: \n",
    "    return \"llava_v0\"\n",
    "  \n",
    "# Common function to proccess images\n",
    "def process_and_prepare_images(image_files, image_processor, model, device):\n",
    "  images = [load_image(image_file) for image_file in image_files]\n",
    "  images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    "  ).to(\n",
    "    device,\n",
    "    dtype=torch.bfloat16\n",
    "  )\n",
    "  image_sizes = [image.size for image in images]\n",
    "  return images_tensor, image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "def load_image(image_input):\n",
    "  # Check if the input is a string (path or URL)\n",
    "  if isinstance(image_input, str):\n",
    "    if image_input.startswith(\"http\") or image_input.startswith(\"https\"):\n",
    "      response = requests.get(image_input)\n",
    "      image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "      image = Image.open(image_input).convert(\"RGB\")\n",
    "  elif isinstance(image_input, Image.Image):\n",
    "    image = image_input\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported image input type\")\n",
    "  return image\n",
    "\n",
    "def eval_model(tokenizer, model, image_processor, context_len, image_file, query, model_name=model_name, sep=\",\", temperature=1.0, num_beams=1, max_tokens=512):\n",
    "  # Model\n",
    "  disable_torch_init()\n",
    "  prompt = create_prompt(query, model, model_name)\n",
    "  \n",
    "  if isinstance(image_file, list):\n",
    "    images_tensor, image_sizes = process_and_prepare_images(image_file, image_processor, model, model.device)\n",
    "  elif isinstance(image_file, str):\n",
    "    images_tensor, image_sizes = process_and_prepare_images([image_file], image_processor, model, model.device)\n",
    "  else:\n",
    "    # if image_file is neither a list or string\n",
    "    images = [image_file]\n",
    "    images_tensor, images_sizes = process_and_prepare_images(images, image_processor, model, model.device)\n",
    "  \n",
    "  input_ids = (\n",
    "    #revisar return_tensors\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .to(model.device)\n",
    "  )\n",
    "  \n",
    "  with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "      input_ids,\n",
    "      images=images_tensor,\n",
    "      image_sizes=image_sizes,\n",
    "      do_sample=temperature != 1.0,\n",
    "      temperature=temperature,\n",
    "      #top_p=top_p,\n",
    "      num_beams=num_beams,\n",
    "      max_new_tokens=max_tokens,\n",
    "      use_cache=True,\n",
    "    )\n",
    "  \n",
    "  outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=False)[0].strip()\n",
    "  print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "#your image url (make sure to use the raw image url form github)\n",
    "image_url = 'https://github.com/TrelisResearch/install-guides/raw/main/knight_and_rook.jpg'\n",
    "\n",
    "#Download the image and open it with PIL\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#Display the image using matplotlib\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Now you can pass the processed image to eval_model\n",
    "eval_model(\n",
    "    tokenizer, \n",
    "    model,\n",
    "    image_processor,\n",
    "    context_len,\n",
    "    image, #use the processed image\n",
    "    \"What do you see in this picture?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuning dataset\n",
    "#preparation for finetuning\n",
    "\n",
    "def tokenize_and_create_labels(example_batch, image_processor, tokenizer, model, device):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch['image']\n",
    "    \n",
    "    #modified\n",
    "    images_tensor, image_sizes = process_and_prepare_images(image_files, image_processor, model, device)\n",
    "    \n",
    "    query = \"What do you see in this picture?\"\n",
    "    \n",
    "    #Tokenize the conversation without the captions to determine with\n",
    "    tokenized_conversations_without_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, None))\n",
    "        for _ in example_batch['caption']\n",
    "    ]\n",
    "    \n",
    "    #tokenize the full conversations with the captions\n",
    "    tokenized_conversations_with_captions = [\n",
    "        tokenizer_imae_token(create_promtp(query, model, model_name, caption))\n",
    "        for caption in example_batch['caption']\n",
    "    ]\n",
    "    \n",
    "    # pad the tokenized conversarions to the same length\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_with_captions])\n",
    "    \n",
    "    #create attetion mask (1 for real tokens and 0 for padding tokens)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "    \n",
    "    # Create the labels tensor with is a copy of input_ids but with\n",
    "    labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
    "    for i, tcwc in enumerate(tokenized_conversations_without_caption):\n",
    "        #Set ignore_index for the tokens corresponding to the conversation\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption)]\n",
    "        \n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"images\": images_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Make sure to define the function outside of the lambda to ensure it's\n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_labels(batch, image_processor, tokenizer,)\n",
    "\n",
    "# load and prepare dataset\n",
    "ds = load_dataset(\"Trelis/chess_pieces\")\n",
    "\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds = ds[\"test\"]\n",
    "\n",
    "#Apply the tranformation function to the dataset\n",
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lora\n",
    "#After specifying the low-rank adapters (LoRA) config, we load the PeftModel using the get_peft_model utility function\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        # \"fc1\", \"fc2\", #for llama,\n",
    "        \"mm_projector\" #for mistral, train instead \"mm_projector\"\n",
    "        \"un_proj\", \"down_proj\",\"gate_proj\" #optionally train more linear\n",
    "        ]\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-training evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#temporarily disable the transformation to access the original data \n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate ove each ecample in the ealuation dataset\n",
    "for i in range(len(eval_ds))\n",
    "    # Accress the original image and caption for the current row\n",
    "    image = eval_ds [i]['image']\n",
    "    caption = eval_ds[i]['caption']\n",
    "    \n",
    "    \n",
    "    #Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  #turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "    \n",
    "    eval_model(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        \"What do you see in this picture?\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
    "    \n",
    "# Re-enable the transformation if needed\n",
    "eval_ds.set_tranform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training, finally we are using the hugging face trainer to finetune the model. Fine-tuning in mixed precision fp16 can lead to overflows. As such, we recommend training in mixed precision bf16 when possible\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_ds is your training dataset prepared as a PyTorch Dataset object \n",
    "batch_size = 4 #Specify the batch size you want to use\n",
    "train_load = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Assuming train_loader is yout Dataloader instance for the trianing dataset\n",
    "for batch in train_loader:\n",
    "    print(batch.keys()) # print the diccionary keys to see what data is included in a  ba\n",
    "    \n",
    "    \n",
    "    # if 'images'is a key, this indicates that images are being loaded\n",
    "    if 'images' in batch:\n",
    "        print(\"Images are included in the DataLoader.\")\n",
    "        print(f\"Batch 'images' shape: {batch['images'].shape}\") #Print the shape of the\n",
    "        \n",
    "    #similarly, check for other expected keys, like 'input_ids' and 'attention_mask'\n",
    "    if 'input_ids' in batch and 'attention_mask' in batch:\n",
    "        # Print the first row of input ids to check out-of-range token IDs\n",
    "        input_ids_first_row = batch['input_ids'][1]\n",
    "        print(f\"First row of 'input_ids': \\n{input_ids_first_row.tolist()}\")\n",
    "        \n",
    "        # # Check if any token IDs are out of range\n",
    "        # vocab_size = tokenizer.vocab_size\n",
    "        # out_of_range_tokens = [token_id for token_id in input_ids_first_row i token_id]\n",
    "        # if out_of_range_tokens:\n",
    "        #   print(f\"Out-of-range token IDs: {out_of_range_tokens})\n",
    "        \n",
    "        # # Decode the first row of input_ids to text, if all token IDs are in range\n",
    "        # If not out_of_range_tokens: \n",
    "        #   decoded_inputs = tokenize.decode(input_ids_first_row, skip_special_toekns=)\n",
    "        #   print(f\"Decoded input tokens: {decoded_inputs}\")\n",
    "        # else:\n",
    "        #   print(\"Cannot decode input_ids due to out-of-range token IDs.\")\n",
    "        \n",
    "        print(\"Text inputs are included in the DataLoader.\")\n",
    "        print(f\"Batch 'input_ids' shape: {batch['input_ids'].shape}\")\n",
    "        print(f\"Batch 'attention_mask' shape: {batch['attention_mask'].shape}\")\n",
    "        \n",
    "        # # Decode the first row of input_ids to text\n",
    "        # decoded_inputs = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=False)\n",
    "        # print(f\"Decoded input tokens: {decoded inputs}\")\n",
    "        \n",
    "        # Print the first row of labels, replacing ingore_index with string '[IGNORE]\n",
    "        labels = batch['labels'][1].tolist()\n",
    "        labels_str =  ['[IGNORE]' if label == -100 else str(label) for label in labels]\n",
    "        print(f\"Labels: {labels_str}\")\n",
    "        \n",
    "        # Print the first row of the attention_mask\n",
    "        attention_mask_str = batch['attention_mask'][1].tolist()\n",
    "        print(f\"Attention mask: {attention_mask_str}\")\n",
    "        \n",
    "    # Optionally, display an image from the batch to visually confirm loading \n",
    "    if 'images' in batch:\n",
    "        image_tensor = batch['images'][1]\n",
    "        print(f\"First Row Image Data type: {image_tensor.dtype}\")\n",
    "        print(f\"First Row Image Shape: {image_tensor.shape}\")\n",
    "        print(f\"First Row Image Value range: [{image_tensor.min()}, {image_tensor.max()}]\")\n",
    "        \n",
    "    break # Only check the first batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sample=[1, 733, 16289, 28793, 28705, 13, 3195, 511, 368, 1032, 297, 456, 575]\n",
    "print(tokenizer.decode(output_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IMAGE_TOKEN_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_QUERY = \"[INST] <image>\\nWhat do you see in this picture? [/INST]\"\n",
    "\n",
    "#image_token_se = DEFAULT_IMAGE_TOKEN\n",
    "\n",
    "# print(f\"image_token_se is: {image_token_se}\")\n",
    "\n",
    "# print(tokenizer_image_token(test_query, tokenizer, IMAGE_TOKEN_INDEX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import CrossEntroppyLoss\n",
    "\n",
    "# def compute_loss(model, imputs, return_outputs=False):\n",
    "#   labels = inputs.pop(\"labels\")\n",
    "#   outputs = model(**inputs)\n",
    "#   logits = outputs.logits\n",
    "#   loss_fct = CrossEntropyLoss(ignore_index=ignore_index)\n",
    "#   loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "#   return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "output_model_name=f\"{model_name}-chess\"\n",
    "\n",
    "training_args = TrainingArgument(\n",
    "    out_dir=output_model_name,\n",
    "    learning_rate=1e-4,\n",
    "    # fp16=True, #for non ampere gpus\n",
    "    bf16=True,\n",
    "    peer_device_train_batch_size=4,\n",
    "    peer_devide_eval_batch_size=6,\n",
    "    gradient_accumulation_step=1,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=2,\n",
    "    ealuation_stategy=\"steps\",\n",
    "    save_steps=0.2,\n",
    "    eval_steps=0.2,   \n",
    "    logging_steps=1\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=3,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=Flase,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    optim=\"adamw_torch\"   \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    # compute_loss=compute_loss, # pass the custom compute\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval after training \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporarily disable the transformation to access the original data\n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate over each example in the evaluation dataset\n",
    "for i in range(len(eval_ds)):\n",
    "    # Access the original image and caption for the current row\n",
    "    image = eval_ds[i]['image']\n",
    "    caption = eval_ds[i]['caption']\n",
    "    \n",
    "    #Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off') #turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "    \n",
    "    eval_model(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        \"What do you see in the picture?\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
    "        \n",
    "#Re-enable the transformation if needed\n",
    "eval_ds.set_transform(lambda batch: ds_transforms(batch, image_processor, tokenizer, model))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxifare-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
