{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install matplotlib -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install q- datasets\n",
    "!pip install transformers -q -U\n",
    "!pip install -q bitsandbytes sentencepiece accelerate loralib\n",
    "!pip install -q -U git+https://github.com/huggingface/perft.git\n",
    "!pip install hf_transfer -q -U\n",
    "!pip install pickleshare -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"LLaVA\"):\n",
    "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
    "else:\n",
    "    print(\"LLaVA directory already exists. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Define the path to the builder.py file\n",
    "file_path = 'LLaVA/llava/model/builder.py'\n",
    "\n",
    "#Read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "#Regular expression to find the block between 'vision_tower = model.get....\n",
    "pattern_block = (\n",
    "    r'(vision_tower = model.get_vision_tower\\(\\)\\n)'\n",
    "    r'.*?' #non-greedy match for any characters\n",
    "    r'(image_processor = vision_tower.image_processor)'\n",
    ")\n",
    "\n",
    "replacement_block = (\n",
    "    r'\\1' # keep starting line unchaged\n",
    "    '     if not vision_tower.is_loader:\\n'\n",
    "    '       print(\\'vision_tower is not loaded so loading it now\\')\\n'\n",
    "    '       vision_tower.load_model(device_map=device_map)\\n'\n",
    "    '       vision_tower.to(deice=device, dtype=torch.bfloat16)\\n'\n",
    "    '     else:\\n'\n",
    "    '       pint(\\'vision_tower is loaded\\')\\n'\n",
    "    r'    \\2' #keep the ending line unchanged\n",
    ")\n",
    "\n",
    "#replace the specific block\n",
    "content = re.sub(pattern_block, replacement_block, content, flegs=re.DOTALL)\n",
    "\n",
    "#Write the modified content back to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(content)\n",
    "print('The script has been updated successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_path = 'LLaVA/llava/model/builder.py'\n",
    "\n",
    "#read the content of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    concept = file.read()\n",
    "    \n",
    "#regular expression to find 'float16' not preceded by 'b'\n",
    "pattern = r'(?<!b)float16'\n",
    "\n",
    "#check if there are any matches\n",
    "if re.search(pattern, content):\n",
    "    #Replace 'float16' with 'bfloat16'\n",
    "    modified_content = re.sub(pattern, 'bfloat16', content)\n",
    "    \n",
    "    #Write the modified contnet back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(modified_content)\n",
    "    \n",
    "    print(\"All necessary instances of floats have been replaced with..\")\n",
    "else:\n",
    "    print('No replacement needed. All instances of float16 already have.. ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can take up to 5 mins\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git pull\n",
    "# !pip install -e . -q\n",
    "\n",
    "!pip install protobuf -q -U\n",
    "!pip install --upgrade Pillow -q\n",
    "!pip install -e \".[train]\" -q\n",
    "!pip install flash-attn --no-build-isolation -q \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#load model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "import transformers\n",
    "from transformers import AutoProcessor, Trainer, TrainingArgument, BitsA\n",
    "import torchvision.transforms as transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = 'liuhaotian/llava-v1.6-mistral-7b'\n",
    "#model_path = \"Trelis/llava-v1.6-mistral-7b-PATCHED\"\n",
    "\n",
    "model_name=get_model_name_from_path(model_path)\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    cache_dir='',\n",
    "    use_flash_attn=True,  \n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.config)\n",
    "#print(tokenizer.pad_token_id)\n",
    "#print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#check dtype of all modules, focusing on those not torch.bfloat16\n",
    "print(\"Modules not torch.bfloat16:\")\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'parameters') and list(module.parameters()):\n",
    "        #check if any parameter of the module is not bfloat16\n",
    "        if any(param.dtype != torch.bfloat16 for para in module.parameters()):\n",
    "            print(f\"{name}: {next(module.parameters()).dtype}\")\n",
    "    else:\n",
    "        #Optionally, acknowledge module without parameters if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "#your image url (make sure to use the raw image url form github)\n",
    "image_url = 'https://github.com/TrelisResearch/install-guides/raw/main/knight_and_rook.jpg'\n",
    "\n",
    "#Download the image and open it with PIL\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#Display the image using matplotlib\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Now you can pass the processed image to eval_model\n",
    "eval_model(\n",
    "    tokenizer, \n",
    "    model,\n",
    "    image_processor,\n",
    "    context_len,\n",
    "    image, #use the processed image\n",
    "    \"What do you see in this picture?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuning dataset\n",
    "#preparation for finetuning\n",
    "\n",
    "def tokenize_and_create_labels(exapme_batch, image_processor, tokenizer)\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = emaple_batch['image']\n",
    "    \n",
    "    images_tensor, image_sizes = process_and_prepare_images(image_files,)\n",
    "    \n",
    "    query = \"What do you see in this picture?\"\n",
    "    \n",
    "    #Tokenize the conversation without the captions to determine with\n",
    "    tokenized_conversations_witout_caption = [\n",
    "        tokenizer_image_token(create_prompt(query, model, model_name, None))\n",
    "        for _ in example_batch['caption']\n",
    "    ]\n",
    "    \n",
    "    #tokenize the full conversations with the captions\n",
    "    tokenized_conversations_with_captions = [\n",
    "        tokenizer_imae_token(create_promtp(query, model, model_name, ))\n",
    "        for caption in example_batch['caption']\n",
    "    ]\n",
    "    \n",
    "    # pad the tokenized conversarions to the same length\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_with_captions])\n",
    "    \n",
    "    #create attetion mask (1 for real tokens and 0 for padding tokens)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "    \n",
    "    # Create the labels tensor with is a copy of input_ids but with\n",
    "    labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
    "    for i, tcwc in enumerate(tokenized_conversations_witout_caption):\n",
    "        #Set ignore_index for the tokens corresponding to the conversation\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_withoout_caption):] = input_ids[i, len(input)]\n",
    "        \n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"images\": images_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Make sure to define the function outside of the lambda to ensure it's\n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_labels(batch, image_processor, tokenizer,)\n",
    "\n",
    "# load and prepare dataset\n",
    "ds = load_dataset(\"Trelis/chess_pieces\")\n",
    "\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds = ds[\"test\"]\n",
    "\n",
    "#Apply the tranformation function to the dataset\n",
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lora\n",
    "#After specifying the low-rank adapters (LoRA) config, we load the PeftModel using the get_peft_model utility function\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        # \"fc1\", \"fc2\", #for llama,\n",
    "        \"mm_projector\" #for mistral, train instead \"mm_projector\"\n",
    "        \"un_proj\", \"down_proj\",\"gate_proj\" #optionally train more linear\n",
    "        ]\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-training evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#temporarily disable the transformation to access the original data \n",
    "eval_ds.reset_format()\n",
    "\n",
    "# Iterate ove each ecample in the ealuation dataset\n",
    "for i in range(len(eval_ds))\n",
    "    # Accress the original image and caption for the current row\n",
    "    image = eval_ds [i]['image']\n",
    "    caption = eval_ds[i]['caption']\n",
    "    \n",
    "    \n",
    "    #Display the image using matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  #turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "    \n",
    "    eval_model(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        image_processor,\n",
    "        context_len,\n",
    "        image,\n",
    "        \"What do you see in this picture?\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
    "    \n",
    "# Re-enable the transformation if needed\n",
    "eval_ds.set_tranform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxifare-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
